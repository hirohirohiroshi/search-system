# -*- coding: utf-8 -*-
import os
import json
import streamlit as st
import gdown
import shutil  # ãƒ•ã‚©ãƒ«ãƒ€å‰Šé™¤ã®ãŸã‚ã«shutilãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from whoosh.index import create_in, open_dir
from whoosh.fields import Schema, TEXT, STORED
from whoosh.qparser import MultifieldParser, AndGroup
from whoosh.analysis import NgramAnalyzer

# --- â–¼ è¨­å®šé …ç›® â–¼ ---
GOOGLE_DRIVE_FILE_ID = "1FGd89dvLBrG8ZZuwlmYbVx-ySX8snAmP"
JSON_FILE_PATH = "downloaded_database.json"
INDEX_DIR = "search_index"
# --- â–² è¨­å®šé …ç›® â–² ---


@st.cache_resource
def get_search_index():
    """
    æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æº–å‚™ã™ã‚‹é–¢æ•°ã€‚
    ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒãªã‘ã‚Œã°ã€Google Driveã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦æ–°è¦ä½œæˆã—ã¾ã™ã€‚
    """
    ja_analyzer = NgramAnalyzer(2)
    schema = Schema(
        sheet_name=TEXT(stored=True, analyzer=ja_analyzer),
        all_content=TEXT(analyzer=ja_analyzer),
        original_data=STORED
    )

    if not os.path.exists(INDEX_DIR):
        print("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æ–°è¦ä½œæˆã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        try:
            print("Google Driveã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™...")
            gdown.download(id=GOOGLE_DRIVE_FILE_ID, output=JSON_FILE_PATH, quiet=False)
            print("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

            with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:
                all_sheets_data = json.load(f)

        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        
        os.mkdir(INDEX_DIR)
        ix = create_in(INDEX_DIR, schema)
        writer = ix.writer()
        
        print("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¦ã„ã¾ã™...")
        with st.spinner("ãƒ‡ãƒ¼ã‚¿ã®ç´¢å¼•ã‚’ä½œæˆä¸­..."): # ã‚¹ãƒ”ãƒŠãƒ¼UIã‚’è¿½åŠ 
            for sheet_name, rows in all_sheets_data.items():
                for row_data in rows:
                    content_list = [str(value) for value in row_data.values()]
                    combined_content = " ".join(content_list)
                    writer.add_document(
                        sheet_name=sheet_name,
                        all_content=combined_content,
                        original_data=row_data
                    )
            writer.commit()
        print("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    else:
        print("æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚")
        ix = open_dir(INDEX_DIR)
        
    return ix


def search(index, query_string):
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ¤œç´¢ã™ã‚‹é–¢æ•°"""
    query_parser = MultifieldParser(
        ["sheet_name", "all_content"],
        schema=index.schema,
        group=AndGroup
    )
    query = query_parser.parse(query_string)
    with index.searcher() as searcher:
        results = [hit.fields() for hit in searcher.search(query, limit=50)]
    return results


# --- â–¼ Streamlitã‚¢ãƒ—ãƒªã®ãƒ¡ã‚¤ãƒ³ã®è¦‹ãŸç›®ã¨æ“ä½œéƒ¨åˆ† â–¼ ---
st.title("ğŸ“‚ é«˜é€Ÿæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã€€ã¯ã‚„ãŠ")

# --- â–¼ å¤‰æ›´ç‚¹ï¼šãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒœã‚¿ãƒ³ã®è¿½åŠ  â–¼ ---
st.write("---")
if st.button("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã‚’å¼·åˆ¶ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥"):
    # search_indexãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã™ã‚Œã°å‰Šé™¤
    if os.path.exists(INDEX_DIR):
        shutil.rmtree(INDEX_DIR)
    # Streamlitã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
    st.cache_resource.clear()
    st.success("ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚ãƒšãƒ¼ã‚¸ã‚’è‡ªå‹•ã§ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")
    # ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã®å†å–å¾—ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚’å¼·åˆ¶ã™ã‚‹
    st.rerun()

st.write("---")

search_index = get_search_index()

search_keyword = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", placeholder="ä¾‹: ç ´ç”£ è³ƒå€Ÿ")

if search_keyword and search_index:
    normalized_keyword = search_keyword.replace('ã€€', ' ').strip()
    keywords_to_highlight = [k for k in set(normalized_keyword.split()) if k]
    
    search_results = search(search_index, normalized_keyword)
    
    st.write("---")
    st.subheader(f"æ¤œç´¢çµæœ: {len(search_results)} ä»¶")

    if not search_results:
        st.warning("è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        for result in search_results:
            with st.container(border=True):
                original_data = result['original_data']
                st.markdown(f"**ã‚·ãƒ¼ãƒˆå:** `{result['sheet_name']}`")
                
                for key, value in original_data.items():
                    display_value = str(value)
                    for keyword in keywords_to_highlight:
                        display_value = display_value.replace(keyword, f"<mark>{keyword}</mark>")
                    
                    st.markdown(f"**{key}:** {display_value}", unsafe_allow_html=True)